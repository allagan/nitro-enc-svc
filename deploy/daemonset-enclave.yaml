# Apply namespace.yaml and rbac.yaml before this manifest:
#   kubectl apply -f deploy/namespace.yaml
#   kubectl apply -f deploy/rbac.yaml
#   kubectl apply -f deploy/daemonset-enclave.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nitro-enclave
  namespace: nitro-enc-svc
  labels:
    app: nitro-enclave
spec:
  selector:
    matchLabels:
      app: nitro-enclave
  # Roll one node at a time so encryption capacity is never fully lost.
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: nitro-enclave
    spec:
      # Only schedule on nodes that have Nitro Enclaves enabled.
      nodeSelector:
        aws.amazon.com/nitro-enclaves: "true"
      tolerations:
        - operator: Exists
      serviceAccountName: nitro-enclave-runner
      containers:
        - name: enclave-runner
          # Image built by CodeBuild and pushed to ECR.
          # Replace <ECR_REGISTRY> and <IMAGE_TAG> with the values from
          # enclave/build-summary.json after each CodeBuild run.
          image: 394582308905.dkr.ecr.us-east-2.amazonaws.com/nitro-enc-svc/dev/runner:dfc4bcc
          # TODO: remove debug-mode override once vsock proxy is wired up
          command:
            - bash
            - -c
            - >-
              sed -i
              's/--enclave-cid "\${ENCLAVE_CID}"/--enclave-cid "\${ENCLAVE_CID}" --debug-mode/'
              /usr/local/bin/run-enclave.sh;
              exec /usr/local/bin/run-enclave.sh
          securityContext:
            privileged: true   # Required: nitro-cli must access /dev/nitro_enclaves
          # The runner container is a thin management shell. The enclave itself
          # uses memory and CPU allocated from the host Nitro Enclaves allocator
          # (configured in /etc/nitro_enclaves/allocator.yaml), not from these limits.
          # hugepages-1Gi MUST be declared so Kubernetes sets the hugetlb cgroup
          # limit to a non-zero value. Without this, kernel 5.7+ sets
          # hugetlb.1GB.rsvd.limit_in_bytes=0 which causes mmap(MAP_HUGE_1GB)
          # to fail with ENOMEM inside the container, even though the host has
          # hugepages reserved. The actual enclave memory (1024 MiB) is allocated
          # from the nitro-enclaves-allocator pool, not from the K8s hugepage
          # resource, but the limit must be >= ENCLAVE_MEMORY_MB for mmap to work.
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
              hugepages-1Gi: "1Gi"
            limits:
              cpu: "500m"
              memory: "2Gi"
              hugepages-1Gi: "1Gi"
          volumeMounts:
            - name: nitro-device
              mountPath: /dev/nitro_enclaves
          env:
            # Memory (MB) to allocate to the enclave from the Nitro allocator.
            # Must be pre-reserved in /etc/nitro_enclaves/allocator.yaml on the host.
            - name: ENCLAVE_MEMORY_MB
              value: "1024"
            # vCPUs to allocate to the enclave. Same pre-reservation requirement.
            - name: ENCLAVE_CPU_COUNT
              value: "2"
            # Vsock CID for this enclave.
            # MUST match VSOCK_PROXY_CID baked into the EIF at build time,
            # and ENCLAVE_CID in all vsock-proxy sidecar containers on this node.
            - name: ENCLAVE_CID
              value: "16"
            # Optional â€” uncomment to override defaults:
            # - name: EIF_PATH
            #   value: "/enclave/nitro-enc-svc.eif"
            # - name: HEALTH_CHECK_INTERVAL
            #   value: "10"
      volumes:
        - name: nitro-device
          hostPath:
            path: /dev/nitro_enclaves
            type: CharDevice
